{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing MelGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "CONTENT_FILES = './input/clp/long'\n",
    "STYLE_FILES = './input/laplus/long'\n",
    "\n",
    "# CONTENT_FILES = './input/boy'\n",
    "# STYLE_FILES = './input/girl'\n",
    "\n",
    "batch_size = 16         #batch size\n",
    "\n",
    "\n",
    "windowSize = 72         # the input width of the model\n",
    "vecLen = 128            # length of vector generated by siamese vector\n",
    "shape = 24              # length of time axis of split specrograms to feed to generator            \n",
    "\n",
    "\n",
    "learning_rate = 0.00005\n",
    "\n",
    "delta = 2.0\n",
    "\n",
    "max_epochs = 10      # maximum epochs\n",
    "\n",
    "load_model_path = './saved_model/waveglow_exp_4'  # path of the pretrained model\n",
    "saved_experiment_path = './saved_model/waveglow_exp_4'   # path of the saved model\n",
    "test_waveform_file = './input/clp/short/small1_5second.wav'\n",
    "\n",
    "if not os.path.exists(saved_experiment_path):\n",
    "    os.mkdir(saved_experiment_path)\n",
    "\n",
    "    \n",
    "plot_spec = True    # plot spectrum when training \n",
    "saved_epoch = 1      # save model period\n",
    "\n",
    "pretrained = False   # If loading the pretrained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 files from ./input/clp/long\n",
      "loaded 1 files from ./input/laplus/long\n",
      "sampleRate: 16000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from WaveGlow.processing import wav2spectrum\n",
    "# from WaveGlow.vars import sample_rate\n",
    "from MelGAN.processing2 import wav2spectrum, sampleRate\n",
    "from utils import loadFile\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "sample_rate = sampleRate\n",
    "\n",
    "def loadData(path):\n",
    "    files = glob(f'{path}/*.wav')\n",
    "    data = []\n",
    "    for file in files:\n",
    "        x = loadFile(file, sample_rate)\n",
    "        data.append(x)\n",
    "    print(f'loaded {len(files)} files from {path}')\n",
    "    return data\n",
    "\n",
    "def transformData(Xs):\n",
    "    ret = []\n",
    "    for x in Xs:\n",
    "        # ret.append(wav2spectrum(x)[:, :, None].numpy())\n",
    "        ret.append(wav2spectrum(x)[:, :, None])\n",
    "    return ret\n",
    "\n",
    "def splitCut(data):\n",
    "    ret = []\n",
    "    mini = 0\n",
    "    miniFinal = 10 * shape\n",
    "    for i in range(len(data) - 1):\n",
    "        mini = min(data[i].shape[1], data[i + 1].shape[1])\n",
    "        if mini >= windowSize and mini < miniFinal:\n",
    "            miniFinal = mini\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        if x.shape[1] >= windowSize:\n",
    "            for n in range(x.shape[1] // miniFinal):\n",
    "                ret.append(x[:, n * miniFinal: n * miniFinal + miniFinal, :])\n",
    "            ret.append(x[:, -miniFinal:, :])\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# Read waveform\n",
    "x_content = loadData(CONTENT_FILES)\n",
    "x_style = loadData(STYLE_FILES)\n",
    "\n",
    "# Transform into spectrum\n",
    "a_content = transformData(x_content)\n",
    "a_style = transformData(x_style)\n",
    "\n",
    "# Split spectrumgrams in chunks with equal size\n",
    "data_content = splitCut(a_content)\n",
    "data_style = splitCut(a_style)\n",
    "\n",
    "\n",
    "print(f'sampleRate: {sample_rate}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 838 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-16 00:03:48.361111: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from WaveGlow.processing import n_mel_channels\n",
    "from MelGAN.processing2 import n_mel_channels\n",
    "# n_mel_channels = hop\n",
    "\n",
    "@tf.function\n",
    "def proc(x):\n",
    "    return tf.image.random_crop(x, size = [n_mel_channels, windowSize, 1])\n",
    "\n",
    "dsa = tf.data.Dataset.from_tensor_slices(data_content).repeat(20).map(proc, num_parallel_calls = tf.data.experimental.AUTOTUNE).shuffle(10000).batch(batch_size, drop_remainder = True)\n",
    "dsb = tf.data.Dataset.from_tensor_slices(data_style).repeat(20).map(proc, num_parallel_calls = tf.data.experimental.AUTOTUNE).shuffle(10000).batch(batch_size, drop_remainder = True)\n",
    "\n",
    "print(f'Load {len(dsa)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huci/.local/lib/python3.8/site-packages/torch_stft/stft.py:46: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Concatenate, Cropping2D\n",
    "from WaveGlow.processing import wav2spectrum\n",
    "from WaveGlow.vars import sample_rate\n",
    "import matplotlib.pyplot as plt\n",
    "from MelGAN.models import *\n",
    "from MelGAN.loss import *\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def crop(x):\n",
    "    x1 = Cropping2D(((0, 0), (0, 2 * (x.shape[2] // 3))))(x)\n",
    "    x2 = Cropping2D(((0, 0), (x.shape[2] // 3, x.shape[2] // 3)))(x)\n",
    "    x3 = Cropping2D(((0, 0), (2 * (x.shape[2] // 3), 0)))(x)\n",
    "    return x1, x2, x3\n",
    "\n",
    "def assemble_image(x1, x2, x3):\n",
    "    x = Concatenate(2)([x1, x2, x3])\n",
    "    return x\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_all(x, y):\n",
    "\n",
    "    x1, x2, x3 = crop(x)\n",
    "    y1, y2, y3 = crop(y)\n",
    "\n",
    "    with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
    "\n",
    "        gen_x_1 = model_G(x1, training = True)\n",
    "        gen_x_2 = model_G(x2, training = True)\n",
    "        gen_x_3 = model_G(x3, training = True)\n",
    "\n",
    "        gen_y_1 = model_G(y1, training = True)\n",
    "        gen_y_2 = model_G(y2, training = True)\n",
    "        gen_y_3 = model_G(y3, training = True)\n",
    "\n",
    "\n",
    "        gen = assemble_image(gen_x_1, gen_x_2, gen_x_3)\n",
    "\n",
    "        iden_gen = model_D(gen, training = True)\n",
    "        iden_ori = model_D(y, training = True)\n",
    "\n",
    "        siam_x_1_gen = model_S(gen_x_1, training = True)\n",
    "        siam_x_2_gen = model_S(gen_x_3, training = True)\n",
    "\n",
    "        siam_x_1 = model_S(x1, training = True)\n",
    "        siam_x_2 = model_S(x3, training = True)\n",
    "\n",
    "        loss_id = (mae(y1, gen_y_1) + mae(y2, gen_y_2) + mae(y3, gen_y_3)) / 3.0\n",
    "\n",
    "        loss_m = loss_travel(siam_x_1, siam_x_1_gen, siam_x_2, siam_x_2_gen)  + loss_siamese(siam_x_1, siam_x_2, delta)\n",
    "\n",
    "        loss_g = g_loss_f(iden_gen)\n",
    "        loss_dr = d_loss_r(iden_ori)\n",
    "        loss_df = d_loss_f(iden_gen)\n",
    "        loss_d = (loss_dr + loss_df) / 2.0\n",
    "\n",
    "        loss_total = loss_g + 10.0 * loss_m + 35.0 * loss_id\n",
    "\n",
    "    grad_gen = tape_gen.gradient(loss_total, model_G.trainable_variables + model_S.trainable_variables)\n",
    "    opt_gen.apply_gradients(zip(grad_gen, model_G.trainable_variables + model_S.trainable_variables))\n",
    "\n",
    "    grad_disc = tape_disc.gradient(loss_d, model_D.trainable_variables)\n",
    "    opt_disc.apply_gradients(zip(grad_disc, model_D.trainable_variables))\n",
    "\n",
    "    return loss_dr, loss_df, loss_g, loss_id\n",
    "\n",
    "# Train Critic only\n",
    "tf.function\n",
    "def train_d(x, y):\n",
    "\n",
    "    x1, x2, x3 = crop(x)\n",
    "    \n",
    "    with tf.GradientTape() as tape_disc:\n",
    "        \n",
    "        gen_x_1 = model_G(x1, training = True)\n",
    "        gen_x_2 = model_G(x2, training = True)\n",
    "        gen_x_3 = model_G(x3, training = True)\n",
    "\n",
    "        gen = assemble_image(gen_x_1, gen_x_2, gen_x_3)\n",
    "\n",
    "        iden_gen = model_D(gen, training = True)\n",
    "        iden_ori = model_D(y, training = True)\n",
    "\n",
    "        loss_dr = d_loss_r(iden_ori)\n",
    "        loss_df = d_loss_f(iden_gen)\n",
    "        loss_d = (loss_dr + loss_df) / 2.0\n",
    "\n",
    "    grad_disc = tape_disc.gradient(loss_d, model_D.trainable_variables)\n",
    "    opt_disc.apply_gradients(zip(grad_disc, model_D.trainable_variables))\n",
    "\n",
    "    return loss_dr, loss_df\n",
    "\n",
    "# Set learning rate\n",
    "def update_lr(lr):\n",
    "    opt_gen.learning_rate = lr\n",
    "    opt_disc.learning_rate = lr\n",
    "\n",
    "def train(epochs, lr = 0.0001, n_save = 3, gupt = 3):\n",
    "\n",
    "    update_lr(lr)\n",
    "\n",
    "    losses = {\n",
    "        'loss_id': [],\n",
    "        'loss_dr': [],\n",
    "        'loss_df': [],\n",
    "        'loss_g': []\n",
    "    }\n",
    "\n",
    "    df_list = []\n",
    "    dr_list = []\n",
    "    g_list = []\n",
    "    id_list = []\n",
    "\n",
    "    c = 0\n",
    "    g = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        \n",
    "        loss_g_epoch = 0.0\n",
    "        loss_id_epoch = 0.0\n",
    "        loss_dr_epoch = 0.0\n",
    "        loss_df_epoch = 0.0\n",
    "\n",
    "        bef = time.time()\n",
    "        for batchi, (x, y) in enumerate(zip(dsa, dsb)):\n",
    "\n",
    "            if batchi % gupt == 0:\n",
    "                dloss_t, dloss_f, gloss, idloss = train_all(x, y)\n",
    "                \n",
    "                loss_dr_epoch += dloss_t\n",
    "                loss_df_epoch += dloss_f\n",
    "                loss_id_epoch += idloss\n",
    "                loss_g_epoch += gloss\n",
    "\n",
    "            else:\n",
    "                dloss_t, dloss_f = train_d(x, y)\n",
    "\n",
    "                loss_dr_epoch += dloss_t\n",
    "                loss_df_epoch += dloss_f\n",
    "\n",
    "\n",
    "            df_list.append(dloss_f)\n",
    "            dr_list.append(dloss_t)\n",
    "            g_list.append(gloss)\n",
    "            id_list.append(idloss)\n",
    "\n",
    "            c += 1\n",
    "            g += 1\n",
    "\n",
    "            if batchi % 600 == 0:\n",
    "                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis = 0)} ', end = '')\n",
    "                print(f'r: {np.mean(dr_list[-g:], axis = 0)}] ', end = '')\n",
    "                print(f'[G loss: {np.mean(g_list[-g:], axis = 0)}] ', end = '')\n",
    "                print(f'[ID loss: {np.mean(id_list[-g:])}] ', end = '')\n",
    "                print(f'[LR: {lr}]')\n",
    "\n",
    "                g = 0\n",
    "            nbatch = batchi\n",
    "\n",
    "        # save training info \n",
    "        losses['loss_g'].append(loss_g_epoch)\n",
    "        losses['loss_id'].append(loss_id_epoch)\n",
    "        losses['loss_dr'].append(loss_dr_epoch)\n",
    "        losses['loss_df'].append(loss_df_epoch)\n",
    "\n",
    "        print(f'Time/Batch {(time.time() - bef) / nbatch}')\n",
    "        print(f'Mean D loss: {np.mean(df_list[-c:], axis = 0)} Mean G loss: {np.mean(g_list[-c:], axis = 0)} Mean ID loss: {np.mean(id_list[-c:], axis = 0)}')\n",
    "        c = 0\n",
    "\n",
    "        if epoch % n_save == 0:\n",
    "\n",
    "            epoch_path = os.path.join(saved_experiment_path, f'epoch_{epoch}')\n",
    "\n",
    "            if not os.path.exists(epoch_path):\n",
    "                os.mkdir(epoch_path)\n",
    "\n",
    "            if os.path.exists(test_waveform_file):\n",
    "                \n",
    "                x_input = loadFile(test_waveform_file, sample_rate)\n",
    "\n",
    "                S_input = wav2spectrum(x_input).detach().cpu().numpy()\n",
    "\n",
    "                \n",
    "\n",
    "                H, W = S_input.shape\n",
    "                S_output = np.zeros((H, W))\n",
    "\n",
    "                for i in range(0, W - shape, shape):\n",
    "                    S_output[:, i: i + shape] = np.array(model_G(S_input[:, i: i + shape].reshape(1, n_mel_channels, shape, 1), training = False)).squeeze()\n",
    "\n",
    "                S_output[:, -shape:] = np.array(model_G(S_input[: , -shape:].reshape(1, n_mel_channels, shape, 1), training = False)).squeeze()\n",
    "\n",
    "                # No spectrum to wave when training generator\n",
    "                # x_output = spectrum2wav(S_output)\n",
    "\n",
    "                # # save generated waveform\n",
    "                # writeFile(os.path.join(epoch_path, 'ori.wav'), x_input, sample_rate)\n",
    "                # writeFile(os.path.join(epoch_path, 'gen.wav'), x_output, sample_rate)\n",
    "\n",
    "\n",
    "                if plot_spec:\n",
    "                    print(S_input.shape)\n",
    "                    print(S_output.shape)\n",
    "                    fig, axs = plt.subplots(ncols = 2)\n",
    "                    axs[0].imshow(np.flip(S_input, -2), cmap = None)\n",
    "                    axs[0].axis('off')\n",
    "                    axs[0].set_title('Source')\n",
    "                    axs[1].imshow(np.flip(S_output, -2), cmap = None)\n",
    "                    axs[1].axis('off')\n",
    "                    axs[1].set_title('Generated')\n",
    "                    plt.show()\n",
    "\n",
    "                model_S.save_weights(os.path.join(epoch_path, 'siam.h5'))\n",
    "                model_G.save_weights(os.path.join(epoch_path, 'gen.h5'))\n",
    "                model_D.save_weights(os.path.join(epoch_path, 'critic.h5'))\n",
    "\n",
    "                print(f'Save model and generate waveform at epoch {epoch}')\n",
    "\n",
    "    return losses, df_list, dr_list, g_list, id_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MelGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=11'>12</a>\u001b[0m opt_gen \u001b[39m=\u001b[39m Adam(learning_rate, \u001b[39m0.5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=12'>13</a>\u001b[0m opt_disc \u001b[39m=\u001b[39m Adam(learning_rate, \u001b[39m0.5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m losses, df_list, dr_list, g_list, id_list \u001b[39m=\u001b[39m train(max_epochs, lr \u001b[39m=\u001b[39;49m learning_rate, n_save \u001b[39m=\u001b[39;49m saved_epoch, gupt \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=17'>18</a>\u001b[0m info \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=18'>19</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: losses,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mg_list\u001b[39m\u001b[39m'\u001b[39m: g_list,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=30'>31</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mpretrained\u001b[39m\u001b[39m'\u001b[39m: pretrained\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=31'>32</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000009vscode-remote?line=33'>34</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(saved_experiment_path, \u001b[39m'\u001b[39m\u001b[39mtrain_info.pkl\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;32m/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, lr, n_save, gupt)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000007vscode-remote?line=124'>125</a>\u001b[0m loss_df_epoch \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000007vscode-remote?line=126'>127</a>\u001b[0m bef \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000007vscode-remote?line=127'>128</a>\u001b[0m \u001b[39mfor\u001b[39;00m batchi, (x, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(dsa, dsb)):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000007vscode-remote?line=129'>130</a>\u001b[0m     \u001b[39mif\u001b[39;00m batchi \u001b[39m%\u001b[39m gupt \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000007vscode-remote?line=130'>131</a>\u001b[0m         dloss_t, dloss_f, gloss, idloss \u001b[39m=\u001b[39m train_all(x, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    765\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    767\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    768\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 749\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    750\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    751\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    752\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    754\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3012\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3011\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3012\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[1;32m   3014\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[1;32m   3015\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3016\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from MelGAN.models import *\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "if pretrained:\n",
    "    model_S, model_G, model_D = load(load_model_path, n_mel_channels, shape, vecLen)\n",
    "else:\n",
    "    model_S, model_G, model_D = build(n_mel_channels, shape, vecLen)\n",
    "\n",
    "\n",
    "opt_gen = Adam(learning_rate, 0.5)\n",
    "opt_disc = Adam(learning_rate, 0.5)\n",
    "\n",
    "losses, df_list, dr_list, g_list, id_list = train(max_epochs, lr = learning_rate, n_save = saved_epoch, gupt = 3)\n",
    "\n",
    "\n",
    "info = {\n",
    "    'loss': losses,\n",
    "    'g_list': g_list,\n",
    "    'df_list': df_list, \n",
    "    'dr_list': dr_list,\n",
    "    'id_list': id_list,\n",
    "    'epochs': max_epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'input_shape': (n_mel_channels, windowSize, 1),\n",
    "    'vecLen': vecLen,\n",
    "    'batch_size': batch_size,\n",
    "    'saved_model_path': saved_experiment_path,\n",
    "    'load_model_path': load_model_path,\n",
    "    'pretrained': pretrained\n",
    "}\n",
    "\n",
    "with open(os.path.join(saved_experiment_path, 'train_info.pkl'), 'wb') as f:\n",
    "    pkl.dump(info, f)\n",
    "\n",
    "model_S.save_weights(os.path.join(saved_experiment_path, 'siam.h5'))\n",
    "model_G.save_weights(os.path.join(saved_experiment_path, 'gen.h5'))\n",
    "model_D.save_weights(os.path.join(saved_experiment_path, 'critic.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from utils import plot_curve\n",
    "# print(losses['loss_g'])\n",
    "\n",
    "# print(losses['loss_df'])\n",
    "\n",
    "# total_loss = np.array(losses['loss_S']) + np.array(losses['loss_G']) + np.array(losses['loss_D'])\n",
    "# print(losses.keys())\n",
    "# plot_curve(losses['loss_D'], losses['loss_G'], total_loss, 'Critic loss', 'Generator loss ', 'Total loss', False)\n",
    "# plot_curve(losses['loss_g'])\n",
    "# print(losses['loss_m'])\n",
    "# print(losses['loss_g'])\n",
    "# print(losses['loss_GS'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WaveGlow.vars import n_mel_channels\n",
    "# from MelGAN.processing import hop\n",
    "# n_mel_channels = hop\n",
    "\n",
    "from MelGAN.models import *\n",
    "import tensorflow as tf\n",
    "\n",
    "model_G = build_generator((n_mel_channels, shape, 1))\n",
    "model_G.load_weights(os.path.join(load_model_path, 'gen.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making WaveGlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huci/.local/lib/python3.8/site-packages/torch_stft/stft.py:46: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  fft_window = pad_center(fft_window, filter_length)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from WaveGlow.processing import spectral_normalize\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, wave, spec, sampleNumber = 10000):\n",
    "\n",
    "        self.spec = []\n",
    "        self.wave = []\n",
    "\n",
    "        for x, s in zip(wave, spec):\n",
    "            \n",
    "            H, W, _ = s.shape\n",
    "            \n",
    "            # crop random samples\n",
    "            indices = np.random.randint(W - crop_length // hop_length, size = sampleNumber)\n",
    "\n",
    "            for i in indices:\n",
    "\n",
    "                j = i * hop_length\n",
    "\n",
    "                S = s[:, i: i + crop_length // hop_length]\n",
    "                S = model_G(S[None, :, :, :], training = False)\n",
    "                S = np.array(S).squeeze()\n",
    "\n",
    "                S = denormalize(S) + ref_level_db\n",
    "                S = librosa.db_to_power(S) \n",
    "\n",
    "                S = spectral_normalize(torch.Tensor(S))\n",
    "\n",
    "                sp = torch.tensor(S, requires_grad = False)\n",
    "                wv = torch.tensor(x[j: j + crop_length], requires_grad = False)\n",
    "\n",
    "                self.spec.append(sp)\n",
    "                self.wave.append(wv)             \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.spec[index], self.wave[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.spec) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use 6144 as sample length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3889/1786455717.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sp = torch.tensor(S, requires_grad = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 10000 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from WaveGlow.vars import n_mel_channels, hop_length\n",
    "from WaveGlow.vars import hop_length\n",
    "from MelGAN.processing2 import denormalize, ref_level_db\n",
    "\n",
    "# from MelGAN.processing import hop\n",
    "# n_mel_channels = hop\n",
    "# hop_length = hop\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl \n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "\n",
    "\n",
    "crop_length = hop_length * shape\n",
    "\n",
    "print(f'Use {crop_length} as sample length')\n",
    "\n",
    "dataset = myDataset(x_style, a_style)\n",
    "\n",
    "\n",
    "print(f'Load {len(dataset)} samples')\n",
    "\n",
    "with open(os.path.join('./dataset/clp2laplus', 'WaveDataset.pkl'), 'wb') as f:\n",
    "    pkl.dump(dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle as pkl\n",
    "from numba import cuda\n",
    "\n",
    "with open(os.path.join('./dataset/clp2laplus', 'WaveDataset.pkl'), 'rb') as f:\n",
    "    device = cuda.get_current_device()\n",
    "    device.reset()\n",
    "    dataset = pkl.load(f)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset, batch_size = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train WaveGlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from IPython.display import clear_output\n",
    "# import torch\n",
    "\n",
    "# PRETRAINED_MODEL = './WaveGlow/waveglow_256channels_universal_v5.pt'\n",
    "# wave = torch.load(PRETRAINED_MODEL)['model']\n",
    "# wave = wave.remove_weightnorm(wave)\n",
    "# clear_output()\n",
    "\n",
    "from WaveGlow.vars import model_config\n",
    "from MelGAN.processing2 import hop\n",
    "from glow import WaveGlow\n",
    "\n",
    "config = model_config\n",
    "# config['n_mel_channels'] = hop\n",
    "\n",
    "wave = WaveGlow(**config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning on WaveGlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huci/.local/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'glow.WaveGlow' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/huci/.local/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.conv.ConvTranspose1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/huci/.local/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/huci/.local/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'glow.WN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/huci/.local/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/huci/.local/lib/python3.8/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'glow.Invertible1x1Conv' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from glow import WaveGlowLoss\n",
    "import torch\n",
    "\n",
    "learning_rate_waveglow = 0.0001\n",
    "\n",
    "max_epochs_waveglow = 2\n",
    "\n",
    "# If gpu is available\n",
    "if torch.cuda.is_available():  \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "loss_metric =  WaveGlowLoss(sigma = 1.0)\n",
    "\n",
    "\n",
    "pretrained_wave_model = './WaveGlow'\n",
    "pretrained_wave = True\n",
    "\n",
    "if pretrained_wave:\n",
    "    # wave.load_state_dict(torch.load(os.path.join(pretrained_wave_model, 'wave.pt')))\n",
    "    wave = torch.load(os.path.join(pretrained_wave_model, 'wave.pt'))['model']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 001/002:  67%|██████▋   | 6688/10000 [40:46<20:11,  2.73batch/s, loss=-6.42]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000011vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m s, x \u001b[39min\u001b[39;00m tepoch:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000011vscode-remote?line=20'>21</a>\u001b[0m     tepoch\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mmax_epochs_waveglow\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000011vscode-remote?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000011vscode-remote?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.2.119/home/huci/new_upload/2022-Intro-to-AI-final-project/MelGlow.ipynb#ch0000011vscode-remote?line=26'>27</a>\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:222\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    220\u001b[0m     p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    221\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m foreach \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse):\n\u001b[0;32m--> 222\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n\u001b[1;32m    223\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     per_device_and_dtype_grads[p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdevice][p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype]\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = optim.Adam( \n",
    "    wave.parameters(),\n",
    "    lr = learning_rate_waveglow\n",
    ")\n",
    "\n",
    "\n",
    "wave.to(device)\n",
    "\n",
    "wave.train()\n",
    "\n",
    "\n",
    "for epoch in range(1, max_epochs_waveglow + 1):\n",
    "\n",
    "    with tqdm(train_loader, unit = 'batch') as tepoch:\n",
    "\n",
    "        for s, x in tepoch:\n",
    "\n",
    "            tepoch.set_description(f'Training Epoch {epoch:03d}/{max_epochs_waveglow:03d}')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            x = x.to(device)\n",
    "            s = s.to(device)\n",
    "        \n",
    "\n",
    "            loss = loss_metric(wave((s, x)))\n",
    "                \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            tepoch.set_postfix({'loss': loss.item()})\n",
    "\n",
    "\n",
    "torch.save(wave.state_dict(), os.path.join(saved_experiment_path, 'wave.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_model_path = './saved_model/clptolapus_4/epoch_2'         # path of the pretrained model\n",
    "saved_experiment_path = './saved_model/waveglow_exp_4'   # path of the saved model\n",
    "test_waveform_file = './input/clp/short/clp_test.wav'   # path of the test waveform\n",
    "\n",
    "shape = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from WaveGlow.processing import wav2spectrum, Denoiser\n",
    "# from WaveGlow.vars import n_mel_channels, sample_rate\n",
    "\n",
    "from MelGAN.processing2 import wav2spectrum, hop, sampleRate\n",
    "from WaveGlow.processing import Denoiser\n",
    "sample_rate = sampleRate\n",
    "n_mel_channels = hop\n",
    "\n",
    "\n",
    "\n",
    "from utils import loadFile, writeFile, plot_spectrum, plot_spectrogram_with_raw_signal\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from MelGAN.models import *\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# load generator\n",
    "model_G = build_generator((n_mel_channels, shape, 1))\n",
    "model_G.load_weights(os.path.join(load_model_path, 'gen.h5'))\n",
    "\n",
    "# load wave file\n",
    "\n",
    "x_test = loadFile(test_waveform_file, sample_rate)\n",
    "\n",
    "# s_test = wav2spectrum(x_test).numpy()\n",
    "s_test = wav2spectrum(x_test)\n",
    "\n",
    "# generate spectrum\n",
    "H, W = s_test.shape\n",
    "s_output = np.zeros((H, W))\n",
    "\n",
    "for i in range(0, W - shape, shape):\n",
    "    s_output[:, i: i + shape] = np.array(model_G(s_test[:, i: i + shape].reshape(1, n_mel_channels, shape, 1), training = False)).squeeze()\n",
    "\n",
    "s_output[:, -shape:] = np.array(model_G(s_test[: , -shape:].reshape(1, n_mel_channels, shape, 1), training = False)).squeeze()\n",
    "\n",
    "# clear memory used for tensorflow \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n",
    "\n",
    "\n",
    "# load WaveGlow model\n",
    "# PRETRAINED_MODEL = './WaveGlow/waveglow_256channels_universal_v5.pt'\n",
    "# wave = torch.load(PRETRAINED_MODEL)['model']\n",
    "# wave = wave.remove_weightnorm(wave)\n",
    "\n",
    "from WaveGlow.vars import model_config\n",
    "from MelGAN.processing2 import hop\n",
    "from glow import WaveGlow\n",
    "\n",
    "config = model_config\n",
    "config['n_mel_channels'] = hop\n",
    "\n",
    "wave = WaveGlow(**config)\n",
    "\n",
    "\n",
    "wave.load_state_dict(torch.load(os.path.join(saved_experiment_path, 'wave.pt')))\n",
    "clear_output()\n",
    "\n",
    "plot_spectrum(s_test, s_output)\n",
    "\n",
    "\n",
    "# If gpu is available\n",
    "if torch.cuda.is_available():  \n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "wave.eval()\n",
    "wave.to(device)\n",
    "\n",
    "model = Denoiser(wave, filter_length = 6 * hop, n_overlap = 6, win_length = 6 * hop, mode = 'special')\n",
    "model.to(device)\n",
    "\n",
    "# transforming to waveform\n",
    "mel = torch.FloatTensor(s_output[None, :, :]).to(device)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    audio = wave.infer(mel, sigma = 1.0)\n",
    "    audio = model(audio)\n",
    "\n",
    "audio = audio.squeeze()\n",
    "audio = audio.cpu().numpy()\n",
    "\n",
    "plot_spectrogram_with_raw_signal(audio, sample_rate)\n",
    "\n",
    "\n",
    "writeFile('./output/waveglow_.wav', audio, sample_rate)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
