{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 雞雞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# CONTENT_FILES = './input/clp/long'\n",
    "# STYLE_FILES = './input/laplus/long'\n",
    "\n",
    "CONTENT_FILES = './input/boy'\n",
    "STYLE_FILES = './input/girl'\n",
    "\n",
    "\n",
    "windowSize = 72         # the input width of the model\n",
    "vecLen = 128            # length of vector generated by siamese vector\n",
    "shape = 24              # length of time axis of split specrograms to feed to generator            \n",
    "\n",
    "batch_size = 16         #batch size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1 files from ./input/boy\n",
      "loaded 1 files from ./input/girl\n",
      "sampleRate: 22050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from WaveGlow.processing import wav2spectrum\n",
    "from WaveGlow.vars import sample_rate\n",
    "from utils import loadFile\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def loadData(path):\n",
    "    files = glob(f'{path}/*.wav')\n",
    "    data = []\n",
    "    for file in files:\n",
    "        x = loadFile(file, sample_rate)\n",
    "        data.append(x)\n",
    "    print(f'loaded {len(files)} files from {path}')\n",
    "    return data\n",
    "\n",
    "def transformData(Xs):\n",
    "    ret = []\n",
    "    for x in Xs:\n",
    "        ret.append(wav2spectrum(x)[:, :, None])\n",
    "    return ret\n",
    "\n",
    "def splitCut(data):\n",
    "    ret = []\n",
    "    mini = 0\n",
    "    miniFinal = 10 * shape\n",
    "    for i in range(len(data) - 1):\n",
    "        mini = min(data[i].shape[1], data[i + 1].shape[1])\n",
    "        if mini >= windowSize and mini < miniFinal:\n",
    "            miniFinal = mini\n",
    "    for i in range(len(data)):\n",
    "        x = data[i]\n",
    "        if x.shape[1] >= windowSize:\n",
    "            for n in range(x.shape[1] // miniFinal):\n",
    "                ret.append(x[:, n * miniFinal: n * miniFinal + miniFinal, :])\n",
    "            ret.append(x[:, -miniFinal:, :])\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "# Read waveform\n",
    "x_content = loadData(CONTENT_FILES)\n",
    "x_style = loadData(STYLE_FILES)\n",
    "\n",
    "# Transform into spectrum\n",
    "a_content = transformData(x_content)\n",
    "a_style = transformData(x_style)\n",
    "\n",
    "# Split spectrumgrams in chunks with equal size\n",
    "data_content = splitCut(a_content)\n",
    "data_style = splitCut(a_style)\n",
    "\n",
    "\n",
    "print(f'sampleRate: {sample_rate}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 0 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from MelGAN.processing import hop\n",
    "\n",
    "@tf.function\n",
    "def proc(x):\n",
    "    return tf.image.random_crop(x, size = [hop, windowSize, 1])\n",
    "\n",
    "dsa = tf.data.Dataset.from_tensor_slices(data_content).repeat(1).map(proc, num_parallel_calls = tf.data.experimental.AUTOTUNE).shuffle(10000).batch(batch_size, drop_remainder = True)\n",
    "dsb = tf.data.Dataset.from_tensor_slices(data_style).repeat(1).map(proc, num_parallel_calls = tf.data.experimental.AUTOTUNE).shuffle(10000).batch(batch_size, drop_remainder = True)\n",
    "\n",
    "print(f'Load {len(dsa)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from MelGAN.processing import sampleRate, wav2spectrum, spectrum2wav\n",
    "from tensorflow.keras.layers import Concatenate, Cropping2D\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import writeFile\n",
    "from MelGAN.models import *\n",
    "from MelGAN.loss import *\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def crop(x):\n",
    "    x1 = Cropping2D(((0, 0), (0, 2 * (x.shape[2] // 3))))(x)\n",
    "    x2 = Cropping2D(((0, 0), (x.shape[2] // 3, x.shape[2] // 3)))(x)\n",
    "    x3 = Cropping2D(((0, 0), (2 * (x.shape[2] // 3), 0)))(x)\n",
    "    return x1, x2, x3\n",
    "\n",
    "def assemble_image(x1, x2, x3):\n",
    "    x = Concatenate(2)([x1, x2, x3])\n",
    "    return x\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_all(x, y):\n",
    "\n",
    "    x1, x2, x3 = crop(x)\n",
    "    y1, y2, y3 = crop(y)\n",
    "\n",
    "    with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
    "\n",
    "        gen_x_1 = model_G(x1, training = True)\n",
    "        gen_x_2 = model_G(x2, training = True)\n",
    "        gen_x_3 = model_G(x3, training = True)\n",
    "\n",
    "        gen_y_1 = model_G(y1, training = True)\n",
    "        gen_y_2 = model_G(y2, training = True)\n",
    "        gen_y_3 = model_G(y3, training = True)\n",
    "\n",
    "\n",
    "        gen = assemble_image(gen_x_1, gen_x_2, gen_x_3)\n",
    "\n",
    "        iden_gen = model_D(gen, training = True)\n",
    "        iden_ori = model_D(y, training = True)\n",
    "\n",
    "        siam_x_1_gen = model_S(gen_x_1, training = True)\n",
    "        siam_x_2_gen = model_S(gen_x_3, training = True)\n",
    "\n",
    "        siam_x_1 = model_S(x1, training = True)\n",
    "        siam_x_2 = model_S(x3, training = True)\n",
    "\n",
    "        loss_id = (mae(y1, gen_y_1) + mae(y2, gen_y_2) + mae(y3, gen_y_3)) / 3.0\n",
    "\n",
    "        loss_m = loss_travel(siam_x_1, siam_x_1_gen, siam_x_2, siam_x_2_gen)  + loss_siamese(siam_x_1, siam_x_2, delta)\n",
    "\n",
    "        loss_g = g_loss_f(iden_gen)\n",
    "        loss_dr = d_loss_r(iden_ori)\n",
    "        loss_df = d_loss_f(iden_gen)\n",
    "        loss_d = (loss_dr + loss_df) / 2.0\n",
    "\n",
    "        loss_total = loss_g + 10.0 * loss_m + 10.0 * loss_id\n",
    "\n",
    "    grad_gen = tape_gen.gradient(loss_total, model_G.trainable_variables + model_S.trainable_variables)\n",
    "    opt_gen.apply_gradients(zip(grad_gen, model_G.trainable_variables + model_S.trainable_variables))\n",
    "\n",
    "    grad_disc = tape_disc.gradient(loss_d, model_D.trainable_variables)\n",
    "    opt_disc.apply_gradients(zip(grad_disc, model_D.trainable_variables))\n",
    "\n",
    "    return loss_dr, loss_df, loss_g, loss_id\n",
    "\n",
    "# Train Critic only\n",
    "tf.function\n",
    "def train_d(x, y):\n",
    "\n",
    "    x1, x2, x3 = crop(x)\n",
    "    \n",
    "    with tf.GradientTape() as tape_disc:\n",
    "        \n",
    "        gen_x_1 = model_G(x1, training = True)\n",
    "        gen_x_2 = model_G(x2, training = True)\n",
    "        gen_x_3 = model_G(x3, training = True)\n",
    "\n",
    "        gen = assemble_image(gen_x_1, gen_x_2, gen_x_3)\n",
    "\n",
    "        iden_gen = model_D(gen, training = True)\n",
    "        iden_ori = model_D(y, training = True)\n",
    "\n",
    "        loss_dr = d_loss_r(iden_ori)\n",
    "        loss_df = d_loss_f(iden_gen)\n",
    "        loss_d = (loss_dr + loss_df) / 2.0\n",
    "\n",
    "    grad_disc = tape_disc.gradient(loss_d, model_D.trainable_variables)\n",
    "    opt_disc.apply_gradients(zip(grad_disc, model_D.trainable_variables))\n",
    "\n",
    "    return loss_dr, loss_df\n",
    "\n",
    "# Set learning rate\n",
    "def update_lr(lr):\n",
    "    opt_gen.learning_rate = lr\n",
    "    opt_disc.learning_rate = lr\n",
    "\n",
    "def train(epochs, lr = 0.0001, n_save = 3, gupt = 3):\n",
    "\n",
    "    update_lr(lr)\n",
    "\n",
    "    losses = {\n",
    "        'loss_id': [],\n",
    "        'loss_dr': [],\n",
    "        'loss_df': [],\n",
    "        'loss_g': []\n",
    "    }\n",
    "\n",
    "    df_list = []\n",
    "    dr_list = []\n",
    "    g_list = []\n",
    "    id_list = []\n",
    "\n",
    "    c = 0\n",
    "    g = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs):\n",
    "        \n",
    "        loss_g_epoch = 0.0\n",
    "        loss_id_epoch = 0.0\n",
    "        loss_dr_epoch = 0.0\n",
    "        loss_df_epoch = 0.0\n",
    "\n",
    "        bef = time.time()\n",
    "        for batchi, (x, y) in enumerate(zip(dsa, dsb)):\n",
    "\n",
    "            if batchi % gupt == 0:\n",
    "                dloss_t, dloss_f, gloss, idloss = train_all(x, y)\n",
    "                \n",
    "                loss_dr_epoch += dloss_t\n",
    "                loss_df_epoch += dloss_f\n",
    "                loss_id_epoch += idloss\n",
    "                loss_g_epoch += gloss\n",
    "\n",
    "            else:\n",
    "                dloss_t, dloss_f = train_d(x, y)\n",
    "\n",
    "                loss_dr_epoch += dloss_t\n",
    "                loss_df_epoch += dloss_f\n",
    "\n",
    "\n",
    "            df_list.append(dloss_f)\n",
    "            dr_list.append(dloss_t)\n",
    "            g_list.append(gloss)\n",
    "            id_list.append(idloss)\n",
    "\n",
    "            c += 1\n",
    "            g += 1\n",
    "\n",
    "            if batchi % 600 == 0:\n",
    "                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis = 0)} ', end = '')\n",
    "                print(f'r: {np.mean(dr_list[-g:], axis = 0)}] ', end = '')\n",
    "                print(f'[G loss: {np.mean(g_list[-g:], axis = 0)}] ', end = '')\n",
    "                print(f'[ID loss: {np.mean(id_list[-g:])}] ', end = '')\n",
    "                print(f'[LR: {lr}]')\n",
    "\n",
    "                g = 0\n",
    "            nbatch = batchi\n",
    "\n",
    "        # save training info \n",
    "        losses['loss_g'].append(loss_g_epoch)\n",
    "        losses['loss_id'].append(loss_id_epoch)\n",
    "        losses['loss_dr'].append(loss_dr_epoch)\n",
    "        losses['loss_df'].append(loss_df_epoch)\n",
    "\n",
    "        print(f'Time/Batch {(time.time() - bef) / nbatch}')\n",
    "        print(f'Mean D loss: {np.mean(df_list[-c:], axis = 0)} Mean G loss: {np.mean(g_list[-c:], axis = 0)} Mean ID loss: {np.mean(id_list[-c:], axis = 0)}')\n",
    "        c = 0\n",
    "\n",
    "        if epoch % n_save == 0:\n",
    "\n",
    "            epoch_path = os.path.join(saved_experiment_path, f'epoch_{epoch}')\n",
    "\n",
    "            if not os.path.exists(epoch_path):\n",
    "                os.mkdir(epoch_path)\n",
    "\n",
    "            if os.path.exists(test_waveform_file):\n",
    "                \n",
    "                x_input = loadFile(test_waveform_file, sampleRate)\n",
    "\n",
    "                S_input = wav2spectrum(x_input)\n",
    "\n",
    "                H, W = S_input.shape\n",
    "                S_output = np.zeros((H, W))\n",
    "\n",
    "                for i in range(0, W - shape, shape):\n",
    "                    S_output[:, i: i + shape] = np.array(model_G(S_input[:, i: i + shape].reshape(1, hop, shape, 1), training = False)).squeeze()\n",
    "\n",
    "                S_output[:, -shape:] = np.array(model_G(S_input[: , -shape:].reshape(1, hop, shape, 1), training = False)).squeeze()\n",
    "\n",
    "\n",
    "                x_output = spectrum2wav(S_output)\n",
    "\n",
    "                # save generated waveform\n",
    "                writeFile(os.path.join(epoch_path, 'ori.wav'), x_input, sampleRate)\n",
    "                writeFile(os.path.join(epoch_path, 'gen.wav'), x_output, sampleRate)\n",
    "\n",
    "\n",
    "                if plot_spec:\n",
    "                    IPython.display.display(IPython.display.Audio(np.squeeze(x_output), rate=sampleRate))\n",
    "                    IPython.display.display(IPython.display.Audio(np.squeeze(x_input), rate=sampleRate))\n",
    "                    fig, axs = plt.subplots(ncols = 2)\n",
    "                    axs[0].imshow(np.flip(S_input, -2), cmap=None)\n",
    "                    axs[0].axis('off')\n",
    "                    axs[0].set_title('Source')\n",
    "                    axs[1].imshow(np.flip(S_output, -2), cmap=None)\n",
    "                    axs[1].axis('off')\n",
    "                    axs[1].set_title('Generated')\n",
    "                    plt.show()\n",
    "\n",
    "                model_S.save_weights(os.path.join(epoch_path, 'siam.h5'))\n",
    "                model_G.save_weights(os.path.join(epoch_path, 'gen.h5'))\n",
    "                model_D.save_weights(os.path.join(epoch_path, 'critic.h5'))\n",
    "\n",
    "                print(f'Save model and generate waveform at epoch {epoch}')\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 0.00001\n",
    "# learning_rate_G = 0.00001\n",
    "# learning_rate_D = 0.000005\n",
    "\n",
    "\n",
    "delta = 2.0\n",
    "\n",
    "\n",
    "max_epochs = 1      # maximum epochs\n",
    "\n",
    "load_model_path = './saved_model/clptolapus_3/epoch_4'  # path of the pretrained model\n",
    "saved_experiment_path = './saved_model/waveglow_test'   # path of the saved model\n",
    "test_waveform_file = './input/clp/short/clp_test.wav'\n",
    "\n",
    "if not os.path.exists(saved_experiment_path):\n",
    "    os.mkdir(saved_experiment_path)\n",
    "\n",
    "    \n",
    "plot_spec = True    # plot spectrum when training \n",
    "saved_epoch = 1      # save model period\n",
    "\n",
    "pretrained = False   # If loading the pretrained model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MelGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "if pretrained:\n",
    "    model_S, model_G, model_D = load(load_model_path, hop, shape, vecLen)\n",
    "else:\n",
    "    model_S, model_G, model_D = build(hop, shape, vecLen)\n",
    "\n",
    "\n",
    "opt_gen = Adam(learning_rate, 0.5)\n",
    "opt_disc = Adam(learning_rate, 0.5)\n",
    "\n",
    "losses = train(max_epochs, lr = learning_rate, n_save = saved_epoch, gupt = 3)\n",
    "\n",
    "\n",
    "info = {\n",
    "    'loss': losses,\n",
    "    'epochs': max_epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'input_shape': (hop, windowSize, 1),\n",
    "    'vecLen': vecLen,\n",
    "    'batch_size': batch_size,\n",
    "    'saved_model_path': saved_experiment_path,\n",
    "    'load_model_path': load_model_path,\n",
    "    'pretrained': pretrained\n",
    "}\n",
    "\n",
    "with open(os.path.join(saved_experiment_path, 'train_info.pkl'), 'wb') as f:\n",
    "    pkl.dump(info, f)\n",
    "\n",
    "model_S.save_weights(os.path.join(saved_experiment_path, 'siam.h5'))\n",
    "model_G.save_weights(os.path.join(saved_experiment_path, 'gen.h5'))\n",
    "model_D.save_weights(os.path.join(saved_experiment_path, 'critic.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from utils import plot_curve\n",
    "# print(losses['loss_g'])\n",
    "\n",
    "# print(losses['loss_df'])\n",
    "\n",
    "# total_loss = np.array(losses['loss_S']) + np.array(losses['loss_G']) + np.array(losses['loss_D'])\n",
    "# print(losses.keys())\n",
    "# plot_curve(losses['loss_D'], losses['loss_G'], total_loss, 'Critic loss', 'Generator loss ', 'Total loss', False)\n",
    "# plot_curve(losses['loss_g'])\n",
    "# print(losses['loss_m'])\n",
    "# print(losses['loss_g'])\n",
    "# print(losses['loss_GS'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveGlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from WaveGlow.vars import n_mel_channels, \n",
    "from WaveGlow.models import WaveGlow\n",
    "\n",
    "PRETRAINED_MODEL = './WaveGlow/waveglow_256channels_universal_v5.pt'\n",
    "\n",
    "\n",
    "\n",
    "wave = WaveGlow(n_mel_channels = n_mel_channels, n_flows = , n_group = , n_early_every = , n_early_size = )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
